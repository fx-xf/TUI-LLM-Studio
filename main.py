#!/usr/bin/env python3
# main.py
import sys
from pathlib import Path
import signal
from prompt_toolkit import prompt
from prompt_toolkit.completion import WordCompleter  # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚
import torch  # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ torch

# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿ÑƒÑ‚Ğ¸
sys.path.append(str(Path(__file__).parent))

from config import MODEL_CONFIG, UI_CONFIG, PROMPT_CONFIG, CHATS_DIR
from model.loader import load_model_simplified, create_dummy_model  # Ğ˜Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚
from model.inference import ModelInference
from chat.manager import ChatManager
from prompts.hyperprompt import get_hyperprompt
from ui.app import ChatUI
from ui.components import UIComponents

def check_model_file():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    model_path = MODEL_CONFIG["model_path"]
    if not model_path.exists():
        print(f"âš ï¸  Ğ¤Ğ°Ğ¹Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {model_path}")
        print(f"ğŸ“ Ğ‘ÑƒĞ´ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ")
        return False
    return True

def load_model():
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    try:
        from config import MODEL_CONFIG
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸš€ Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° {device}...")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ĞµÑĞ»Ğ¸ Ñ„Ğ°Ğ¹Ğ» GGUF ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚
        gguf_path = MODEL_CONFIG["model_path"]
        if gguf_path.exists() and gguf_path.suffix == ".gguf":
            print("âš ï¸  GGUF Ñ„Ğ°Ğ¹Ğ» Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½, Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· transformers")
        
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ
        model, metadata = load_model_simplified(gguf_path, device)
        
        print(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ°!")
        print(f"   ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°: {metadata.get('model_type', 'unknown')}")
        print(f"   Ğ¡Ğ»Ğ¾Ğ¸: {metadata['num_layers']}")
        print(f"   ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: {metadata['hidden_size']}")
        
        return model, metadata
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
        print("ğŸ”„ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ°...")
        
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ dummy Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
        model, metadata = create_dummy_model("cpu")
        return model, metadata

def setup_hyperprompt():
    """ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°"""
    template_name = PROMPT_CONFIG.get("default_template", "default")
    hyperprompt = get_hyperprompt(template_name)
    hyperprompt.system_template = hyperprompt.system_template.replace(
        "{{ system_message }}", 
        PROMPT_CONFIG["system_role"]
    )
    return hyperprompt

def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ dummy)
    check_model_file()
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    model, metadata = load_model()
    inference = ModelInference(model, metadata)
    
    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
    hyperprompt = setup_hyperprompt()
    
    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ğ° Ñ‡Ğ°Ñ‚Ğ¾Ğ²
    chat_manager = ChatManager(CHATS_DIR)
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞº UI
    ui = ChatUI(chat_manager, inference, hyperprompt)
    
    # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ²
    def signal_handler(sig, frame):
        print("\nğŸšª Ğ’Ñ‹Ñ…Ğ¾Ğ´ Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ...")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    
    # ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ
    UIComponents.clear_screen()
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               LLM TUI - DeepSeek Coder                     â•‘
â•‘                                                             â•‘
â•‘  ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {MODEL_CONFIG['model_path'].name[:30]:<30} â•‘
â•‘  Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {'CUDA' if torch.cuda.is_available() else 'CPU':>44} â•‘
â•‘  Ğ“Ğ¾Ñ€ÑÑ‡Ğ¸Ğµ ĞºĞ»Ğ°Ğ²Ğ¸ÑˆĞ¸: Ctrl+H - Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞºĞ°                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ
    action = prompt("ĞĞ°Ñ‡Ğ°Ñ‚ÑŒ [N]Ğ¾Ğ²Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚, [L]oad Ñ‡Ğ°Ñ‚ Ğ¸Ğ»Ğ¸ [Q]uit? ", 
                   completer=WordCompleter(["n", "l", "q"]), default="n").lower()
    
    if action == "l":
        chats = chat_manager.get_chat_list()
        if chats:
            UIComponents.print_header("Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ‡Ğ°Ñ‚Ñ‹")
            for i, chat in enumerate(chats, 1):
                print(f"{i}. {chat['title']} ({chat['message_count']} ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹)")
            try:
                choice = int(prompt("Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ½Ğ¾Ğ¼ĞµÑ€: ")) - 1
                if 0 <= choice < len(chats):
                    chat_manager.load_chat(chats[choice]["id"])
            except:
                pass
    elif action == "q":
        return
    
    # Ğ—Ğ°Ğ¿ÑƒÑĞº UI
    try:
        ui.run()
    except Exception as e:
        print(f"âŒ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("ğŸ‘‹ Ğ”Ğ¾ ÑĞ²Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ!")

if __name__ == "__main__":
    main()