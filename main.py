#!/usr/bin/env python3
import sys
import traceback
import logging
import torch
from pathlib import Path

# --------------------- 1.  –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ --------------------- #
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.FileHandler("llm-tui.log", mode="w", encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)
log = logging.getLogger("main")

# --------------------- 2.  –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ --------------------- #
def main() -> None:
    log.info("===  LLM-TUI  –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è  ===")
    try:
        log.info("–ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π...")
        from app.app import LLMTUIApp
        from app.core.config import Config
        from app.utils.model_downloader import download_model

        log.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥–∞...")
        Config.initialize()

        if torch.cuda.is_available():
            log.info("üöÄ GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω: %s", torch.cuda.get_device_name())
            log.info("üìä –î–æ—Å—Ç—É–ø–Ω–æ –ø–∞–º—è—Ç–∏: %.1f –ì–ë",
                     torch.cuda.get_device_properties(0).total_memory / 1024**3)
        else:
            log.info("‚ö†Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU (–º–µ–¥–ª–µ–Ω–Ω–æ)")

        log.info("–°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏...")
        download_model()

        log.info("–°–æ–∑–¥–∞—ë–º LLMTUIApp...")
        app = LLMTUIApp()
        log.info("LLMTUIApp —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")

        log.info("–í—ã–∑—ã–≤–∞–µ–º app.run()...")
        app.run()
        log.info("app.run() –≤–µ—Ä–Ω—É–ª —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")

    except Exception as exc:
        log.exception("–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê ‚Äì –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —É–ø–∞–ª–æ")
        traceback.print_exc(file=sys.stdout)
        input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")


if __name__ == "__main__":
    main()